Introduction: We chose to create a Facial Expression / Emotion Recognition classification model. Furthermore, once the emotion is recognized we will apply a filter to the image that colors the background of the image with a color representative of the identified emotion. For example, if our model recognizes an angry face in an image, then the background of the image would turn red, leaving only the person's face to be visible.
 
Challenges: 
In terms of the deep learning models, specifically CNNs, one of the hardest parts has been training and tuning their parameters efficiently. One of our team members' computers has significantly limited GPUs, so the process of testing models with different hyperparameters (e.g filters, dense units, kernel sizes, and strides) has been slow and oftentimes has crashed their computer. Additionally, when the training did not crash, the models needed to train at a low batch size so that the computer could successfully run the computations, so it would take a very long time. Additionally, for one of our team members the most challenging part has been similar to the first team member in terms of the time the models take to learn. Given that a large number of epochs was chosen, the model took very long to train and every small change in the modelâ€™s architecture involved waiting for a long time to see the results. Moreover, the biggest struggle was choosing an optimal number of epochs, batch size, and layers to include in the model. 
In terms of applying a color to the image after facial detection, the hardest part has been to find a mechanism to select the background of the image and change the color of that. At the beginning we wanted to change the color of the face, but then we realized that it would be difficult to maintain the features of the face with a different tone for the skin (that would correspond to the emotion.) Thus, we decided to change it so that what would change is the color of the background of the image. To do this, we found a trained deep learning model [https://towardsdatascience.com/change-the-background-of-any-image-with-5-lines-of-code-23a0ef10ce9a] that finds a body within the image and cuts out the background. Then, the background could be changed so that it is a particular color. 
 
Insights: 
As noted previously, at this point we have already successfully collected, cleaned, and preprocessed our data, which can be found in the data folder. Moreover, in the preprocess.py, one can use the get_data function to get the x_train, y_train, x_test, y_test tensors. We have also created two visualizations, one that showcases 9 image samples and the other the distribution of emotion classification data points in the train and test datasets. From the latter, it is apparent that the images are not evenly distributed per emotion and have some large discrepancies. For example, in the train dataset there are 7215 images classified as Happy while there are 3171 and 436 classified as Surprise and Disgust respectively. Given this, we plan to introduce data augmentation strategies when training our model (as discussed in the Plan section). 
Additionally, we have designed, trained, and validated 4 CNNs with different yet similar architectures:

CNN-1 Architecture: 4[Convolution-BatchNormalization-ReLU-Dropout-MaxPool] - Flatten - 3[Dense-BatchNormalization-ReLU] - SoftMax.
CNN-2 Architecture: 3[Convolution-BatchNormalization-ReLU-MaxPool]  Flatten - 2[Dense-BatchNormalization-ReLU] - SoftMax.
CNN-3 Architecture: 4[Convolution-BatchNormalization-ReLU-Dropout-MaxPool] - Flatten - 2[Dense-BatchNormalization-ReLU] - SoftMax.
CNN-4 Architecture: 5[Convolution-ReLU-Dropout-MaxPool] - Flatten - 3[Dense-BatchNormalization-ReLU] - SoftMax

For the first three CNNs, a combination of filters, kernels and dropout rates for each convolution layer and a combination of dense units were tested and tuned. Our findings highlighted how, for the first model, the highest validation accuracy, 0.57, was achieved with a learning rate of 0.001, dropout rate of 0.2, filters of 16, 32, 64, 128 for each convolutional layer with a 5x5 kernel and dense units of 128, 64, 32 for each hidden layer. For the second model, the highest validation accuracy, 0.54, was achieved with a learning rate of 0.001, filters of 64, 128, 256 for each convolutional layer with a 3x3 kernel and desne units of 256, 128 for each hidden layer. For the third model, the highest validation accuracy, 0.56, was achieved with a learning rate of 0.001, dropout rate of 0.2, filters of 64, 128, 512, 512 for each convolutional layer where the 1st, 3rd, and 4th layers had a 3x3 kernel and the 2nd a 5x5, and dense units of 512, 256 for each hidden layer. For the last model, the highest validation accuracy, 0.58, was achieved with a learning rate of 0.01, dropout rate of 0.2, filters of 256 ,256, 128, 64, 64 for the layers respectively, with all of the layers having a kernel of 3x3, and dense units of 512, 256, and 128 for each hidden layer. One key insight we saw was that the three models that used dropout layers performed better than the model without it. This is because without such layers CNN-2 overfit during training. We will continue to tune out dropout layers as we finalized a single model.
Lastly, we have designed and implemented the second part of our project that recognizes an individual's face in an image and/or live feed, and colors the background leaving only the face visible. Using the pretrained model found online we realized that using live feed would take too long because the image had to be passed through the model to find the background. Thus, we changed the live detection so that instead you can take a picture using your computer's camera. Once the picture is taken, our model will then find the corresponding emotion and after knowing this emotion the pretrained model found online will select the background and change the color to that associated with the emotion found in our model. 
 
Plan: 
We are currently on track with our project as we have cleaned and processed our data, implemeted and validated different CNNs, and finished the facial detection with background color application. In addition to digital posters, oral presentation recording, and final write up for the final check in, we need to dedicate some time to design a CNN by integrating the insight's of our different models and validations. After that, given the uneven distribution of emotion classification in the datasets and validation accuracy slightly below our target goal of 0.65, we need to dedicate some time to tune our final CNN's parameters and, more importantly, test the use of data augmentation and image transformation strategies. If these three approaches result in a model that is computationally heavy for our machines (as has already been a challenge), then we would consider training and testing our model through GCP and Google Cloud. Nonetheless, through these three approaches, we are confident that our model will reach our target goal.
Last but not least, once our model is finalized, we need to dedicate some time to integrate it with the part of our project's pipeline that detects an individual's face and applies a color on the background. With such integration, we envision having a program that receives an image and/or snapshot of a life feed, classifies the person's facial emotion, and then paints the image's background with the emotion's corresponding color. 